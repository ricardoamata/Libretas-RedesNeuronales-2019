{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una red neuronal multicapa simple usando TensorFlow\n",
    "\n",
    "\n",
    "[**Ricardo E. Alvarado Mata**](), 8 de abril de 2019.\n",
    "\n",
    "\n",
    "\n",
    "En esta libreta se muestra el ejemplo básico para una red multicapa sencilla\n",
    "aplicada al conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Esta libreta es básicamente una traducción del ejemplo\n",
    "desarrollado por [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos\n",
    "\n",
    "Primero cargamos los archivos que se utilizan para el aprendizaje. Para otro tipo de problemas, es necesario hacer un proceso conocido como *Data Wrangling*, que normalmente se realiza con la ayuda de *Pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c3d55fec490c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que un aprendizaje tenga sentido es necesario tener bien separado un conjunto de datos de aprendizaje y otro de prueba (en caso de grandes conjuntos de datos es la opción). Como vemos tanto las imágenes como las etiquetas están separados en archivos de datos y de aprendizaje.\n",
    "\n",
    "El objeto `mnist` es un objeto tensorflow que contiene 3 objetos tipo tensorflow: *test*, *train* y *validation*, los cuales a su vez contienen *ndarrays* de *numpy*. La estructura es la misma para cada conjunto de datos. Veamos su estructura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-69586a02d799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de images: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de epochs_completed: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_completed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de labels: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de nest_batch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tipo de num_examples: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Tipo de images: {}\".format(type(x_train)))\n",
    "print(\"Tipo de epochs_completed: {}\".format(type(mnist.train.epochs_completed)))\n",
    "print(\"Tipo de labels: {}\".format(type(y_train)))\n",
    "print(\"Tipo de nest_batch: {}\".format(type(mnist.train.next_batch)))\n",
    "print(\"Tipo de num_examples: {}\".format(type(mnist.train.num_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como generar el conjunto de datos para ser utilizado dentro de TensorFlow es objeto de otra libreta. Por el momento concentremonos en como hacer una red neuronal rápido y sin dolor.\n",
    "\n",
    "Sin embargo, vamos a ver unos cuantos datos que nos pueden ser de útilidad para la construcción de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del ndarray con las imágenes: (55000, 784)\n",
      "Forma del ndarray con las etiquetas: (55000, 10)\n",
      "-------------------------------------------------------------------------------\n",
      "Número de imagenes de entrenamiento: 55000\n",
      "Tamaño de las imagenes: 784\n",
      "Clases diferentes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Forma del ndarray con las imágenes: {}\".format(mnist.train.images.shape))\n",
    "print(\"Forma del ndarray con las etiquetas: {}\".format(mnist.train.labels.shape))\n",
    "print(\"-\" * 79)\n",
    "print(\"Número de imagenes de entrenamiento: {}\".format(mnist.train.images.shape[0]))\n",
    "print(\"Tamaño de las imagenes: {}\".format(mnist.train.images.shape[1]))\n",
    "print(\"Clases diferentes: {}\".format(mnist.train.labels.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una red neuronal lo más genérica posible y que pdamos reutilizar en otros proyectos, vamos a establecer los parámetros base independientemente de la inicialización de la red, independientemente de la forma en que construimos la red. \n",
    "\n",
    "Comencemos por establecer una función genérica que nos forme una red neuronal con dos capas ocultas. No agrego más comentarios porque, con la experiencia de las libretas anteriores, la construcción de la red neuronal se explica sola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_neuronal_dos_capas_ocultas(x, pesos, sesgos):\n",
    "    \"\"\"\n",
    "    Genera una red neuronal de dos capas para usar en TensorFlow\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    pesos: un diccionario con tres etiquetas: 'h1', 'h2' y 'ho'\n",
    "           en donde cada una es una tf.Variable conteniendo una \n",
    "           matriz de dimensión [num_neuronas_capa_anterior, num_neuronas_capa]\n",
    "                  \n",
    "    sesgos: un diccionario con tres etiquetas: 'b1', 'b2' y 'bo'\n",
    "            en donde cada una es una tf.Variable conteniendo un\n",
    "            vector de dimensión [numero_de_neuronas_capa]\n",
    "                   \n",
    "    Devuelve\n",
    "    --------\n",
    "    Un ops de tensorflow que calcula la salida de una red neuronal\n",
    "    con dos capas ocultas, y activaciones RELU.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Primera capa oculta con activación ReLU\n",
    "    capa_1 = tf.matmul(x, pesos['h1'])\n",
    "    capa_1 = tf.add(capa_1, sesgos['b1'])\n",
    "    capa_1 = tf.nn.relu(capa_1)\n",
    "    \n",
    "    # Segunda capa oculta con activación ReLU\n",
    "    capa_2 = tf.matmul(capa_1, pesos['h2'])\n",
    "    capa_2 = tf.add(capa_2, sesgos['b2'])\n",
    "    capa_2 = tf.nn.relu(capa_2)\n",
    "    \n",
    "    # Capa de salida con activación lineal\n",
    "    # En Tensorflow la salida es siempre lineal, y luego se especifica\n",
    "    # la función de salida a la hora de calcularla como vamos a ver \n",
    "    # más adelante\n",
    "    capa_salida = tf.matmul(capa_2, pesos['ho']) + sesgos['bo']\n",
    "    return capa_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora necesitamos poder generar los datos de entrada a la red neuronal de\n",
    "alguna manera posible. Afortunadamente sabemos exactamente que necesitaos, así\n",
    "que vamos a hacer una función que nos genere las variables de peso y sesgo.\n",
    "\n",
    "Por el momento, y muy a la brava, solo vamos a generarlas con números aletorios con una \n",
    "distribución $\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_pesos(entradas, n1, n2, salidas):\n",
    "    \"\"\"\n",
    "    Genera un diccionario con pesos  \n",
    "    para ser utilizado en la función red_neuronal_dos_capas_ocultas\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    entradas: Número de neuronas en la capa de entrada\n",
    "    \n",
    "    n1: Número de neuronas en la primer capa oculta\n",
    "    \n",
    "    n2: Número de neuronas en la segunda capa oculta\n",
    "    \n",
    "    salidas: Número de neuronas de salida\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Dos diccionarios, uno con los pesos por capa y otro con los sesgos por capa\n",
    "    \n",
    "    \"\"\"\n",
    "    pesos = {\n",
    "        'h1': tf.Variable(tf.random_normal([entradas, n1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n1, n2])),\n",
    "        'ho': tf.Variable(tf.random_normal([n2, salidas]))\n",
    "    }\n",
    "    \n",
    "    sesgos = {\n",
    "        'b1': tf.Variable(tf.random_normal([n1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n2])),\n",
    "        'bo': tf.Variable(tf.random_normal([salidas]))\n",
    "    }\n",
    "    \n",
    "    return pesos, sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos establecer los parámetros de la topología de la red neuronal. \n",
    "Tomemos en cuenta que estos prámetros los podríamos haber establecido desde\n",
    "la primer celda, si el fin es estar variando los parámetros para escoger los que \n",
    "ofrezcan mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entradas = 784  #  Lo sabemos por la inspección que hicimos a mnist\n",
    "num_salidas = 10    # Ídem\n",
    "\n",
    "# Aqui es donde podemos jugar\n",
    "num_neuronas_capa_1 = 256\n",
    "num_neuronas_capa_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡A construir la red! Para esto vamos a necesitar crear las entradas\n",
    "con un placeholder, y crear nuestra topología de red neuronal.\n",
    "\n",
    "Observa que la dimensión de x será [None, num_entradas], lo que significa que \n",
    "la cantidad de renglones es desconocida (o variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# La entrada a la red neuronal\n",
    "x = tf.placeholder(\"float\", [None, num_entradas])\n",
    "\n",
    "# Los pesos y los sesgos\n",
    "w, b = inicializa_pesos(num_entradas, num_neuronas_capa_1, num_neuronas_capa_2, num_salidas)\n",
    "\n",
    "# Crea la red neuronal\n",
    "estimado = red_neuronal_dos_capas_ocultas(x, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecería que ya está todo listo. Sin ambargo falta algo muy importante: No hemos explicado\n",
    "ni cual es el criterio de error (loss) que vamos a utilizar, ni cual va a ser el método de\n",
    "optimización (aprendizaje) que hemos decidido aplicar.\n",
    "\n",
    "Primero definamos el costo que queremos minimizar, y ese costo va a estar en función de lo\n",
    "estimado con lo real, por lo que necesitamos otra entrada de datos para los datos de salida.\n",
    "\n",
    "Sin ningun lugar a dudas, el costo que mejor describe este problema es el de *softmax*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-86b9e8f4cd9d>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Creamos la variable de datos de salida conocidos\n",
    "y = tf.placeholder(\"float\", [None, num_salidas])\n",
    "\n",
    "#  Definimos la función de costo\n",
    "costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=estimado, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora definimos que función de aprendizaje vamos a utilizar. Existen muchas funciones\n",
    "de aprendizaje en tensorflow, las cuales se pueden consultar en `tf.train.`. Entre las\n",
    "existentes podemos ver algunas conocidas del curso como descenso de gradiente simple,\n",
    "momento, rprop, rmsprop entre otras. Casi todas las funciones de optimización (aprendizaje)\n",
    "acaban su nombre con `Optimize`.\n",
    "\n",
    "En este caso vamos a usar un método comocido como el *algoritmo de Adam* el cual \n",
    "se puede consultar [aqui](http://arxiv.org/pdf/1412.6980.pdf). El metodo utiliza dos calculos\n",
    "de momentos diferentes, y por lo visto genera resultados muy interesantes desde el punto \n",
    "de vista práctico.\n",
    "\n",
    "¿Cual es el mejor método? Pues esto es en función de tu problema y de la cantidad de datos que tengas.\n",
    "Lo mejor es practicar con varios métodos para entender sus ventajas y desventajas.\n",
    "\n",
    "En todo caso el método de optimización requiere que se le inicialice con una tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecutar la sesión usando mini-batches\n",
    "\n",
    "Ahora, ya que la red neuronal está lista vamos a ejecutar la red utilizando el algoritmo de\n",
    "Adam pero en forma de mini-batches. Con el fin de tener control sobre el problema, vamos a establecer un número máximo de epoch (ciclos de aprendizaje), el tamaño de los mini-batches, y cada cuandos epoch \n",
    "quisieramos ver como está evolucionando la red neuronal.\n",
    "\n",
    "Como entrenar una red neuronal no tiene sentido, si no es porque la queremos usar para reconocer,\n",
    "no tendría sentido entrenarla y luego perderla y tener que reentrenar en cada ocasión. Recuerda que cuando\n",
    "se cierra la sesión se borra todo lo que se tenía en memoria. \n",
    "\n",
    "Para esto vamos a usar una ops especial llamada `Saver`, que permite guardar en un archivo la red neuronal y \n",
    "después utilizarla en otra sesión (en otro script, computadora, ....).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todo se ejecuta dentro de una sesión, no es posible hacerlo por partes (si usamos el \n",
    "`with` que debería ser la única forma en la que iniciaramos una sesión). Por lo tanto procuraré dejar comentado el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0            Costo: 162.7347904794868\n",
      "Epoch: 1            Costo: 41.537032917196115\n",
      "Epoch: 2            Costo: 26.044397225379942\n",
      "Epoch: 3            Costo: 17.951468424202844\n",
      "Epoch: 4            Costo: 13.007051344960935\n",
      "Epoch: 5            Costo: 9.62961664669105\n",
      "Epoch: 6            Costo: 7.102127709011984\n",
      "Epoch: 7            Costo: 5.361154221393599\n",
      "Epoch: 8            Costo: 3.9330285310651427\n",
      "Epoch: 9            Costo: 2.88094773416836\n",
      "Epoch: 10           Costo: 2.171126623229836\n",
      "Epoch: 11           Costo: 1.6916796030342358\n",
      "Epoch: 12           Costo: 1.2516160166498285\n",
      "Epoch: 13           Costo: 1.0789389541352874\n",
      "Epoch: 14           Costo: 0.8462320798350921\n",
      "Epoch: 15           Costo: 0.621920164743991\n",
      "Epoch: 16           Costo: 0.6143611576059482\n",
      "Epoch: 17           Costo: 0.5896930299659344\n",
      "Epoch: 18           Costo: 0.4654387623333575\n",
      "Epoch: 19           Costo: 0.43625355116517317\n",
      "Epoch: 20           Costo: 0.36944642829471364\n",
      "Epoch: 21           Costo: 0.44769777258043514\n",
      "Epoch: 22           Costo: 0.31587043003222615\n",
      "Epoch: 23           Costo: 0.29881935148587696\n",
      "Epoch: 24           Costo: 0.34290505366678914\n",
      "Epoch: 25           Costo: 0.3262367938734587\n",
      "Epoch: 26           Costo: 0.30860145164274005\n",
      "Epoch: 27           Costo: 0.24284415972748666\n",
      "Epoch: 28           Costo: 0.21467886554148072\n",
      "Epoch: 29           Costo: 0.22662137983062772\n",
      "Se acabaron los epochs, saliendo de la sesión de tensorflow.\n"
     ]
    }
   ],
   "source": [
    "numero_epochs = 30\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora vamos a revisar que tan bien realizó el aprendizaje cuando se aplica la red adatos que\n",
    "no se usaron para entrenamiento. Para esto vamos a utilizar dos ops extas: una \n",
    "para definir la operaración de datos bien estimados o mal estimados, y otra para\n",
    "calcular el promedio de datos bien estimados. Para calcular los datos bien estimados vamos a utilizar `tf.cast` que permite ajustar los tipos\n",
    "al tipo tensor. \n",
    "\n",
    "Recuerda que, al haber definido `prediccion_correcta` y `precision` como ops de tensorflow *después* de la ultima vez que usaste dentro de una sesión la ops `saver`, estas operaciones no se guardaron en memoria. Si quieres conservarlas, debes de definirlas *antes* de ejecutar el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a abrir una nueva sesión, vamos a restaurar los valores de la sesión anterior,\n",
    "y vamos a ejecutar el grafo con el fin de evaluar la ops precision, pero ahora con el\n",
    "diccionario de alimentación con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from rnn2.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n\nCaused by op 'save_1/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-aa370a4d5b3a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable/Adam_2 not found in checkpoint\n\t [[{{node save_1/RestoreV2}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1276\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1277\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n\nCaused by op 'save_1/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-aa370a4d5b3a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mnames_to_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[0;34m(checkpoint_path)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   object_graph_string = reader.get_tensor(\n\u001b[0;32m-> 1591\u001b[0;31m       checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[1;32m   1592\u001b[0m   object_graph_proto = (\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m    369\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[0;32m--> 370\u001b[0;31m                                           status)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6ac06f9478e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchivo_modelo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n\u001b[1;32m      5\u001b[0m                                                         y: mnist.test.labels})\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0;31m# a helpful message (b/110263146)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[0;32m-> 1292\u001b[0;31m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n\nCaused by op 'save_1/RestoreV2', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 370, in dispatch_queue\n    yield self.process_one()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 346, in wrapper\n    runner = Runner(result, future, yielded)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1080, in __init__\n    self.run()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-aa370a4d5b3a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in __init__\n    self.build()\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 844, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 881, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 513, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 332, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 580, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/ricardo/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam_2 not found in checkpoint\n\t [[node save_1/RestoreV2 (defined at <ipython-input-19-aa370a4d5b3a>:2) ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contesta las siguientes preguntas\n",
    "\n",
    "1. ¿Que pasa si aumenta el número de epochs? ¿Cuando deja de ser util aumentar los epoch?\n",
    "\n",
    "2. ¿Que pasa si aumentas o disminuyes la tasa de aprendizaje?\n",
    "\n",
    "3. Utiliza al menos otros 2 métodos de optimización (existentes en Tensorflow), ajústalos y compáralos. ¿Cual de los métodos te gusta más y porqué preferirías uno sobre los otros?\n",
    "\n",
    "4. ¿Que pasa si cambias el tamaño de los minibatches?\n",
    "\n",
    "5. ¿Como harías si dejaste a medias un proceso de aprendizaje (en 10 epochs por ejemplo) y quisieras entrenar la red 10 epoch más, y mañana quisieras entrenarla otros 10 epoch más?\n",
    "\n",
    "**Para contestar las preguntas, agrega cuantas celdas con comentarios y con códgo sean necesarias.** Aprovecha que las libretas de *Jupyter* te permite hacerte una especie de tutorial personalizado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
